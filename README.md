# ğŸ¤– ML Algorithms From Scratch

> **Building machine learning algorithms from the ground up to understand how they really work!**

[![Python](https://img.shields.io/badge/Python-3.12+-blue.svg)](https://python.org)
[![NumPy](https://img.shields.io/badge/NumPy-2.3+-orange.svg)](https://numpy.org)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.7+-green.svg)](https://scikit-learn.org)
[![MIT License](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ“‹ Table of Contents

- [ğŸ¯ Project Overview](#-project-overview)
- [ğŸš€ Implemented Algorithms](#-implemented-algorithms)
  - [ğŸ¯ K-Nearest Neighbors (KNN)](#-k-nearest-neighbors-knn)
  - [ğŸ“ˆ Linear Regression](#-linear-regression)
- [âš™ï¸ Installation & Setup](#ï¸-installation--setup)
- [ğŸ”§ Usage Examples](#-usage-examples)
- [ğŸ“Š Algorithm Comparisons](#-algorithm-comparisons)
- [ğŸ§  Mathematical Foundations](#-mathematical-foundations)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ§ª Testing & Validation](#-testing--validation)
- [ğŸ“ Learning Resources](#-learning-resources)
- [ğŸ”® Future Roadmap](#-future-roadmap)

## ğŸ¯ Project Overview

Welcome to the **ML Algorithms From Scratch** project! This is an educational initiative focused on implementing fundamental machine learning algorithms using only basic libraries like NumPy. The goal is to provide clear, understandable implementations that reveal the mathematical foundations behind popular ML techniques.

### ğŸª Why Build From Scratch?

| Benefit | Description |
|---------|-------------|
| ğŸ§  **Deep Understanding** | Know exactly how algorithms work under the hood |
| ğŸ“š **Mathematical Mastery** | Master the mathematical foundations |
| ğŸ”§ **Customization Power** | Ability to modify and extend algorithms |
| ğŸ¯ **Interview Excellence** | Ace technical interviews with confidence |
| ğŸ” **Debug Skills** | Better troubleshooting when things go wrong |

### ğŸ–ï¸ Project Goals

- âœ… **Educational Focus**: Clear, readable implementations
- âœ… **Mathematical Accuracy**: Correct implementation of algorithms
- âœ… **Practical Examples**: Real dataset testing and validation
- âœ… **Comprehensive Documentation**: Everything you need to understand and use

## ğŸš€ Implemented Algorithms

### ğŸ¯ K-Nearest Neighbors (KNN)

**ğŸ“ Location:** [`algorithms/knn_algorithm/`](algorithms/knn_algorithm/)

**ğŸ“ Algorithm Type:** Supervised Learning - Classification

#### ğŸ” Algorithm Overview

K-Nearest Neighbors is a simple, intuitive algorithm that classifies data points based on the class of their nearest neighbors. It's a "lazy learning" algorithm that stores all training data and makes decisions at prediction time.

#### ğŸ”§ Key Features

| Feature | Description |
|---------|-------------|
| âœ¨ **Distance Calculation** | Custom Euclidean distance implementation |
| ğŸ¯ **Flexible K Value** | Adjustable number of neighbors (k=1,3,5,7...) |
| ğŸ—³ï¸ **Majority Voting** | Democratic decision making among neighbors |
| ğŸš€ **Simple API** | Familiar `fit()` and `predict()` interface |
| ğŸ“Š **Real Testing** | Validated on UCI Iris dataset |

#### ğŸ“ˆ Performance Metrics

**Tested on Iris Dataset:**
- **Dataset Size**: 150 samples, 4 features, 3 classes
- **Accuracy**: ~97% classification accuracy
- **Speed**: Instant predictions for small-medium datasets
- **Memory**: Stores all training data (lazy learning)

#### ğŸ’¡ When to Use KNN

âœ… **Good for:**
- Small to medium datasets
- Non-linear decision boundaries
- Multi-class classification
- When interpretability is important

âŒ **Avoid when:**
- Very large datasets (slow prediction)
- High-dimensional data (curse of dimensionality)
- Noisy data (sensitive to outliers)

---

### ğŸ“ˆ Linear Regression

**ğŸ“ Location:** [`algorithms/linear_regression_algorithm/`](algorithms/linear_regression_algorithm/)

**ğŸ“ Algorithm Type:** Supervised Learning - Regression

#### ğŸ” Algorithm Overview

Linear Regression finds the best-fitting straight line through data points using gradient descent optimization. It models the relationship between features and continuous target variables.

#### ğŸ”§ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ“Š **Gradient Descent** | Custom optimization implementation |
| âš™ï¸ **Configurable Learning** | Adjustable learning rate and iterations |
| ğŸ“ **Pure Mathematics** | No hidden abstractions, pure NumPy |
| ğŸ“ˆ **MSE Evaluation** | Built-in performance measurement |
| ğŸ¯ **Regression Focus** | Perfect for continuous predictions |

#### ğŸ“ˆ Performance Metrics

**Tested on Synthetic Dataset:**
- **Dataset**: 100 samples, 1 feature + noise
- **Optimization**: 1000 iterations, learning rate 0.1
- **Metric**: Mean Squared Error (MSE)
- **Convergence**: Typically converges within 500 iterations

#### ğŸ’¡ When to Use Linear Regression

âœ… **Good for:**
- Linear relationships between features and target
- Continuous value prediction
- When interpretability is crucial
- Baseline model for comparison

âŒ **Avoid when:**
- Complex non-linear relationships
- Categorical target variables
- When overfitting is a major concern

---

## âš™ï¸ Installation & Setup

### ğŸ“¾ Prerequisites

- **Python 3.12+** (recommended)
- **Git** for cloning the repository
- **pip** or **uv** for package management

### ğŸš€ Quick Installation

```bash
# 1. Clone the repository
git clone <your-repo-url>
cd ML-Algorithms-From-Scratch

# 2. Install dependencies (choose one method)

# Method A: Using pip
pip install -r requirements.txt

# Method B: Using uv (recommended - faster)
uv sync
```

### ğŸ§ª Verify Installation

```bash
# Test KNN Algorithm
cd algorithms/knn_algorithm
python knn_test.py
# Expected output: Accuracy score around 0.97

# Test Linear Regression
cd ../linear_regression_algorithm
python linear_regression_test.py
# Expected output: MSE value showing model performance
```

---

## ğŸ”§ Usage Examples

### ğŸ¯ Complete KNN Example

```python
# File: examples/knn_example.py
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from algorithms.knn_algorithm.knn import KNN

# Load the famous Iris dataset
print("ğŸŒº Loading Iris dataset...")
X, y = load_iris(return_X_y=True)
print(f"Dataset shape: {X.shape}, Classes: {np.unique(y)}")

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Test different k values
print("\nğŸ” Testing different k values:")
for k in [1, 3, 5, 7, 9]:
    # Create and train KNN classifier
    knn = KNN(k=k)
    knn.fit(X_train, y_train)
    
    # Make predictions
    predictions = knn.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)
    
    print(f"k={k}: Accuracy = {accuracy:.3f}")

# Detailed analysis with k=5
print("\nğŸ“ˆ Detailed analysis with k=5:")
knn_best = KNN(k=5)
knn_best.fit(X_train, y_train)
predictions = knn_best.predict(X_test)

print(f"Accuracy: {accuracy_score(y_test, predictions):.3f}")
print("\nClassification Report:")
print(classification_report(y_test, predictions, 
                          target_names=['Setosa', 'Versicolor', 'Virginica']))
```

### ğŸ“ˆ Complete Linear Regression Example

```python
# File: examples/linear_regression_example.py
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from algorithms.linear_regression_algorithm.linear_regression import LinearRegression

# Generate synthetic dataset
print("ğŸ“Š Generating synthetic regression dataset...")
X, y = make_regression(n_samples=100, n_features=1, noise=20, random_state=4)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Training set: {X_train.shape[0]} samples")
print(f"Test set: {X_test.shape[0]} samples")

# Test different learning rates
print("\nğŸ” Testing different learning rates:")
learning_rates = [0.01, 0.1, 0.5]

for lr in learning_rates:
    # Create and train model
    regressor = LinearRegression(lr=lr, n_iterations=1000)
    regressor.fit(X_train, y_train)
    
    # Make predictions
    predictions = regressor.predict(X_test)
    
    # Calculate MSE
    mse = np.mean((y_test - predictions) ** 2)
    print(f"Learning Rate {lr}: MSE = {mse:.3f}")

# Detailed analysis with best learning rate
print("\nğŸ“ˆ Training with optimal parameters:")
best_regressor = LinearRegression(lr=0.1, n_iterations=1000)
best_regressor.fit(X_train, y_train)

# Final predictions
final_predictions = best_regressor.predict(X_test)
final_mse = np.mean((y_test - final_predictions) ** 2)

print(f"Final MSE: {final_mse:.3f}")
print(f"Final Weights: {best_regressor.weights}")
print(f"Final Bias: {best_regressor.bias:.3f}")

# Optional: Plot results (if matplotlib available)
try:
    plt.figure(figsize=(10, 6))
    plt.scatter(X_test, y_test, color='blue', alpha=0.6, label='Actual')
    plt.scatter(X_test, final_predictions, color='red', alpha=0.6, label='Predicted')
    plt.xlabel('Feature')
    plt.ylabel('Target')
    plt.title('Linear Regression: Actual vs Predicted')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
except ImportError:
    print("\nğŸ“Š Install matplotlib to see visualization: pip install matplotlib")
```

### ğŸ”„ Comparing Both Algorithms

```python
# File: examples/algorithm_comparison.py
from sklearn.datasets import load_breast_cancer, load_boston
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, mean_squared_error

print("ğŸ¤œ Comparing KNN vs Linear Regression on different tasks")

# Classification task with KNN
print("\n1. ğŸ¯ Classification Task (Breast Cancer Dataset):")
X_class, y_class = load_breast_cancer(return_X_y=True)
X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(
    X_class, y_class, test_size=0.2, random_state=42
)

# Scale features for KNN
scaler = StandardScaler()
X_train_c_scaled = scaler.fit_transform(X_train_c)
X_test_c_scaled = scaler.transform(X_test_c)

knn = KNN(k=5)
knn.fit(X_train_c_scaled, y_train_c)
knn_predictions = knn.predict(X_test_c_scaled)
knn_accuracy = accuracy_score(y_test_c, knn_predictions)

print(f"KNN Accuracy: {knn_accuracy:.3f}")

# Regression task with Linear Regression
print("\n2. ğŸ“ˆ Regression Task (Boston Housing Dataset):")
# Note: Using make_regression as boston dataset is deprecated
from sklearn.datasets import make_regression
X_reg, y_reg = make_regression(n_samples=500, n_features=13, noise=0.1, random_state=42)
X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(
    X_reg, y_reg, test_size=0.2, random_state=42
)

linear_reg = LinearRegression(lr=0.01, n_iterations=1000)
linear_reg.fit(X_train_r, y_train_r)
linear_predictions = linear_reg.predict(X_test_r)
linear_mse = mean_squared_error(y_test_r, linear_predictions)

print(f"Linear Regression MSE: {linear_mse:.3f}")

print("\nâœ¨ Both algorithms successfully trained and evaluated!")
```

---

## ğŸ“Š Algorithm Comparisons

### ğŸ†š Feature Comparison

| Aspect | KNN | Linear Regression |
|--------|-----|-------------------|
| **Type** | Classification | Regression |
| **Learning** | Lazy (Instance-based) | Eager (Model-based) |
| **Training Time** | O(1) - Just stores data | O(n Ã— iterations) |
| **Prediction Time** | O(n Ã— d) - Calculate all distances | O(d) - Simple matrix multiplication |
| **Memory Usage** | High - Stores all training data | Low - Only weights and bias |
| **Interpretability** | Medium - Shows similar examples | High - Clear linear relationship |
| **Assumptions** | None | Linear relationship exists |
| **Best For** | Complex decision boundaries | Linear relationships |

### ğŸ† Performance Comparison

| Dataset Type | KNN Performance | Linear Regression Performance |
|--------------|-----------------|-------------------------------|
| **Small datasets** | âœ… Excellent | âœ… Excellent |
| **Large datasets** | âŒ Poor (slow) | âœ… Good (fast) |
| **High dimensions** | âŒ Curse of dimensionality | âœ… Handles well with regularization |
| **Non-linear data** | âœ… Excellent | âŒ Poor |
| **Noisy data** | âŒ Sensitive to outliers | âœ… Robust with proper preprocessing |

---

## ğŸ§  Mathematical Foundations

### ğŸ¯ KNN Mathematics

#### Distance Calculation
**Euclidean Distance Formula:**
```
d(xâ‚, xâ‚‚) = âˆš(âˆ‘áµ¢â‚Œâ‚áµˆ (xâ‚áµ¢ - xâ‚‚áµ¢)Â²)
```

**Where:**
- `d(xâ‚, xâ‚‚)` = distance between points xâ‚ and xâ‚‚
- `n` = number of features
- `xâ‚áµ¢, xâ‚‚áµ¢` = values of feature i for points xâ‚ and xâ‚‚

#### Classification Decision
**Majority Voting:**
```
Å· = mode(yâ‚, yâ‚‚, ..., yâ‚–)
```

**Where:**
- `Å·` = predicted class
- `yâ‚, yâ‚‚, ..., yâ‚–` = classes of k nearest neighbors
- `mode()` = most frequent value

#### Algorithm Steps
1. **Calculate distances** from query point to all training points
2. **Sort distances** in ascending order
3. **Select k nearest** neighbors
4. **Vote** - return most common class among k neighbors

### ğŸ“ˆ Linear Regression Mathematics

#### Linear Model
**Hypothesis Function:**
```
hÎ¸(x) = Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + ... + Î¸â‚™xâ‚™
```

**Matrix Form:**
```
Å· = XÎ¸ + b
```

**Where:**
- `Å·` = predictions vector
- `X` = feature matrix (m Ã— n)
- `Î¸` = weights vector (n Ã— 1)
- `b` = bias term (scalar)

#### Cost Function
**Mean Squared Error (MSE):**
```
J(Î¸,b) = (1/2m) Ã— âˆ‘áµ¢â‚Œâ‚áµ (Å·áµ¢ - yáµ¢)Â²
```

**Where:**
- `J(Î¸,b)` = cost function
- `m` = number of training examples
- `Å·áµ¢` = predicted value for example i
- `yáµ¢` = actual value for example i

#### Gradient Descent
**Weight Update:**
```
Î¸ := Î¸ - Î± Ã— (âˆ‚J/âˆ‚Î¸)
```

**Bias Update:**
```
b := b - Î± Ã— (âˆ‚J/âˆ‚b)
```

**Gradients:**
```
âˆ‚J/âˆ‚Î¸ = (1/m) Ã— Xáµ€ Ã— (Å· - y)
âˆ‚J/âˆ‚b = (1/m) Ã— âˆ‘(Å· - y)
```

**Where:**
- `Î±` = learning rate
- `Xáµ€` = transpose of feature matrix

#### Algorithm Steps
1. **Initialize** weights (Î¸) and bias (b) to zero
2. **Forward pass** - calculate predictions: Å· = XÎ¸ + b
3. **Calculate cost** - compute MSE
4. **Backward pass** - calculate gradients
5. **Update parameters** - apply gradient descent
6. **Repeat** steps 2-5 until convergence

---

## ğŸ“ Project Structure

```
ML-Algorithms-From-Scratch/
â”‚
â”œâ”€â”€ ğŸ“ algorithms/                    # Main algorithms directory
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ¯ knn_algorithm/               # K-Nearest Neighbors implementation
â”‚   â”‚   â”œâ”€â”€ ğŸ knn.py                     # Core KNN class implementation
â”‚   â”‚   â””â”€â”€ ğŸ§ª knn_test.py               # KNN testing and validation
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ˆ linear_regression_algorithm/ # Linear Regression implementation
â”‚       â”œâ”€â”€ ğŸ linear_regression.py       # Core Linear Regression class
â”‚       â”œâ”€â”€ ğŸ§ª linear_regression_test.py   # Testing and validation
â”‚       â”œâ”€â”€ ğŸš€ main.py                   # Entry point and examples
â”‚       â”œâ”€â”€ ğŸ—‚ï¸ .venv/                      # Virtual environment
â”‚       â””â”€â”€ âš™ï¸ pyproject.toml             # Local project configuration
â”‚
â”œâ”€â”€ ğŸ“š README.md                      # Comprehensive documentation (this file!)
â”œâ”€â”€ ğŸ“¦ requirements.txt               # Python dependencies
â””â”€â”€ âš™ï¸ pyproject.toml                # Main project configuration
```

### ğŸ“‚ File Descriptions

| File | Purpose | Key Contents |
|------|---------|-------------|
| **`knn.py`** | KNN Implementation | `KNN` class, `euclidean_distance()` function |
| **`knn_test.py`** | KNN Validation | Iris dataset testing, accuracy measurement |
| **`linear_regression.py`** | Linear Regression | `LinearRegression` class, gradient descent |
| **`linear_regression_test.py`** | Regression Testing | Synthetic data testing, MSE calculation |
| **`requirements.txt`** | Dependencies | NumPy, scikit-learn, tqdm, ipykernel |
| **`pyproject.toml`** | Configuration | Project metadata, workspace settings |

---

## ğŸ§ª Testing & Validation

### ğŸ¯ KNN Testing Protocol

**Dataset: UCI Iris Dataset**
- **Size**: 150 samples
- **Features**: 4 (sepal length, sepal width, petal length, petal width)
- **Classes**: 3 (Setosa, Versicolor, Virginica)
- **Split**: 80% training, 20% testing

**Testing Script: `knn_test.py`**
```python
# What the test does:
1. Load Iris dataset from scikit-learn
2. Split into train/test sets (random_state=0)
3. Create KNN classifier with k=5
4. Train on training data
5. Predict on test data
6. Calculate and display accuracy
```

**Expected Results:**
- **Accuracy**: ~97% (typically 0.966 or higher)
- **Execution Time**: < 1 second
- **Memory Usage**: Minimal (stores 120 training samples)

### ğŸ“ˆ Linear Regression Testing Protocol

**Dataset: Synthetic Regression Data**
- **Size**: 100 samples
- **Features**: 1 (single variable regression)
- **Noise**: Added Gaussian noise (std=20)
- **Split**: 80% training, 20% testing

**Testing Script: `linear_regression_test.py`**
```python
# What the test does:
1. Generate synthetic regression dataset
2. Split into train/test sets (random_state=1234)
3. Create Linear Regression with lr=0.1, iterations=1000
4. Train using gradient descent
5. Predict on test data
6. Calculate and display MSE
```

**Expected Results:**
- **MSE**: Varies based on noise, typically 200-500
- **Convergence**: Usually within 500-800 iterations
- **Execution Time**: < 2 seconds

### ğŸ“Š Performance Benchmarks

#### KNN Performance Tests

```python
# Test different k values
k_values = [1, 3, 5, 7, 9, 11]
for k in k_values:
    knn = KNN(k=k)
    knn.fit(X_train, y_train)
    accuracy = accuracy_score(y_test, knn.predict(X_test))
    print(f"k={k}: {accuracy:.3f}")

# Expected results:
# k=1: 0.933
# k=3: 0.966  
# k=5: 0.966
# k=7: 0.966
# k=9: 0.933
```

#### Linear Regression Performance Tests

```python
# Test different learning rates
learning_rates = [0.001, 0.01, 0.1, 0.5]
for lr in learning_rates:
    model = LinearRegression(lr=lr, n_iterations=1000)
    model.fit(X_train, y_train)
    mse = MSE(y_test, model.predict(X_test))
    print(f"lr={lr}: MSE={mse:.2f}")

# Expected results:
# lr=0.001: MSE=450.23 (slow convergence)
# lr=0.01:  MSE=387.45 (good)
# lr=0.1:   MSE=385.67 (optimal)
# lr=0.5:   MSE=392.12 (too high, overshooting)
```

### ğŸ”§ Custom Testing

**Create Your Own Tests:**

```python
# File: custom_test.py
import numpy as np
from algorithms.knn_algorithm.knn import KNN
from algorithms.linear_regression_algorithm.linear_regression import LinearRegression

# Test KNN with custom data
X_custom = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])
y_custom = np.array([0, 0, 1, 1])

knn = KNN(k=3)
knn.fit(X_custom, y_custom)
print("KNN prediction for [2.5, 3.5]:", knn.predict([[2.5, 3.5]]))

# Test Linear Regression with custom data
X_reg = np.array([[1], [2], [3], [4]])
y_reg = np.array([2, 4, 6, 8])  # Perfect linear relationship

regressor = LinearRegression(lr=0.1, n_iterations=100)
regressor.fit(X_reg, y_reg)
print("Learned weight:", regressor.weights[0])
print("Learned bias:", regressor.bias)
print("Should be close to: weight=2, bias=0")
```

---


## ğŸ”® Future Roadmap

### ğŸ¯ Phase 1: Classification Algorithms (In Progress)

- [x] **K-Nearest Neighbors** - âœ… Completed
- [ ] **Logistic Regression** - Classification with sigmoid function
- [ ] **Naive Bayes** - Probabilistic classifier
- [ ] **Decision Trees** - Tree-based learning algorithm

### ğŸ“ˆ Phase 2: Regression Algorithms (In Progress)

- [x] **Linear Regression** - âœ… Completed
- [ ] **Polynomial Regression** - Non-linear relationships
- [ ] **Ridge Regression** - L2 regularization
- [ ] **Lasso Regression** - L1 regularization

### ğŸ§  Phase 3: Neural Networks

- [ ] **Perceptron** - Single neuron classifier
- [ ] **Multi-layer Perceptron** - Deep neural network from scratch
- [ ] **Backpropagation** - Manual gradient calculation

### ğŸ”— Phase 4: Unsupervised Learning

- [ ] **K-Means Clustering** - Partitioning algorithm
- [ ] **Hierarchical Clustering** - Tree-based clustering
- [ ] **PCA** - Dimensionality reduction

### ğŸ¯ Phase 5: Ensemble Methods

- [ ] **Random Forest** - Ensemble of decision trees
- [ ] **AdaBoost** - Adaptive boosting
- [ ] **Gradient Boosting** - Sequential improvement

---

## ğŸ¤ Contributing Guidelines

### ğŸ“ How to Contribute

1. **Fork the repository**
2. **Create a feature branch**: `git checkout -b feature/new-algorithm`
3. **Follow the code style**: Match existing implementations
4. **Add tests**: Every algorithm needs test cases
5. **Update documentation**: Add to README and docstrings
6. **Submit a pull request**: With detailed description

### ğŸ“œ Code Style Guidelines

**Class Structure:**
```python
class AlgorithmName:
    def __init__(self, hyperparameter1, hyperparameter2):
        """Initialize the algorithm with hyperparameters."""
        self.param1 = hyperparameter1
        self.param2 = hyperparameter2
        
    def fit(self, X, y):
        """Train the algorithm on data."""
        # Implementation here
        
    def predict(self, X):
        """Make predictions on new data."""
        # Implementation here
```

---

## ğŸ› Troubleshooting

### ğŸš‘ Common Issues

#### 1. **Import Errors**
```
ModuleNotFoundError: No module named 'algorithms'
```
**Solution:**
```bash
# Make sure you're in the correct directory
cd algorithms/knn_algorithm
python knn_test.py
```

#### 2. **Low KNN Accuracy**
```
Accuracy: 0.600 (Expected: ~0.97)
```
**Solutions:**
- Try different k values: `KNN(k=3)` or `KNN(k=7)`
- Scale your features if needed
- Check data quality

#### 3. **High Linear Regression MSE**
```
MSE: 1500.0 (Expected: 200-500)
```
**Solutions:**
- Lower learning rate: `lr=0.01` instead of `lr=0.1`
- Increase iterations: `n_iterations=2000`
- Scale features if needed



