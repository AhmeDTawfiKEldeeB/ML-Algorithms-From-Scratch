# ğŸ¤– ML Algorithms From Scratch

> **Building machine learning algorithms from the ground up to understand how they really work!**

[![Python](https://img.shields.io/badge/Python-3.12+-blue.svg)](https://python.org)
[![NumPy](https://img.shields.io/badge/NumPy-2.3+-orange.svg)](https://numpy.org)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.7+-green.svg)](https://scikit-learn.org)
[![MIT License](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ“‹ Table of Contents

- [ğŸ¯ Project Overview](#-project-overview)
- [ğŸš€ Implemented Algorithms](#-implemented-algorithms)
  - [ğŸ¯ K-Nearest Neighbors (KNN)](#-k-nearest-neighbors-knn)
  - [ğŸ“ˆ Linear Regression](#-linear-regression)
  - [ğŸ§  Logistic Regression](#-logistic-regression)
  - [ğŸ§  Naive Bayes](#-naive-bayes)
  - [ğŸŒ³ Decision Tree](#-decision-tree)
  - [ğŸŒ² Random Forest](#-random-forest)
  - [ğŸ“Š Principal Component Analysis (PCA)](#-principal-component-analysis-pca)  
- [ğŸ’» Code Showcase](#-code-showcase)
- [âš™ï¸ Installation & Setup](#ï¸-installation--setup)
- [ğŸ“Š Algorithm Comparisons](#-algorithm-comparisons)
- [ğŸ§  Mathematical Foundations](#-mathematical-foundations)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ”® Future Roadmap](#-future-roadmap)

## ğŸ¯ Project Overview

Welcome to the **ML Algorithms From Scratch** project! This is an educational initiative focused on implementing fundamental machine learning algorithms using only basic libraries like NumPy. The goal is to provide clear, understandable implementations that reveal the mathematical foundations behind popular ML techniques.

### ğŸª Why Build From Scratch?

| Benefit | Description |
|---------|-------------|
| ğŸ§  **Deep Understanding** | Know exactly how algorithms work under the hood |
| ğŸ“š **Mathematical Mastery** | Master the mathematical foundations |
| ğŸ”§ **Customization Power** | Ability to modify and extend algorithms |
| ğŸ¯ **Interview Excellence** | Ace technical interviews with confidence |
| ğŸ” **Debug Skills** | Better troubleshooting when things go wrong |

### ğŸ–ï¸ Project Goals

- âœ… **Educational Focus**: Clear, readable implementations
- âœ… **Mathematical Accuracy**: Correct implementation of algorithms
- âœ… **Practical Examples**: Real dataset testing and validation
- âœ… **Comprehensive Documentation**: Everything you need to understand and use

## ğŸš€ Implemented Algorithms

### ğŸ¯ K-Nearest Neighbors (KNN)

**ğŸ“ Location:** [`algorithms/knn_algorithm/`](algorithms/knn_algorithm/)

**ğŸ“ Algorithm Type:** Supervised Learning - Classification

#### ğŸ” Algorithm Overview

K-Nearest Neighbors is a simple, intuitive algorithm that classifies data points based on the class of their nearest neighbors. It's a "lazy learning" algorithm that stores all training data and makes decisions at prediction time.

#### ğŸ”§ Key Features

| Feature | Description |
|---------|-------------|
| âœ¨ **Distance Calculation** | Custom Euclidean distance implementation |
| ğŸ¯ **Flexible K Value** | Adjustable number of neighbors (k=1,3,5,7...) |
| ğŸ—³ï¸ **Majority Voting** | Democratic decision making among neighbors |
| ğŸš€ **Simple API** | Familiar `fit()` and `predict()` interface |
| ğŸ“Š **Real Testing** | Validated on UCI Iris dataset |

#### ğŸ“ˆ Performance Metrics

**Tested on Iris Dataset:**
- **Dataset Size**: 150 samples, 4 features, 3 classes
- **Accuracy**: ~97% classification accuracy
- **Speed**: Instant predictions for small-medium datasets
- **Memory**: Stores all training data (lazy learning)

#### ğŸ’¡ When to Use KNN

âœ… **Good for:**
- Small to medium datasets
- Non-linear decision boundaries
- Multi-class classification
- When interpretability is important

âŒ **Avoid when:**
- Very large datasets (slow prediction)
- High-dimensional data (curse of dimensionality)
- Noisy data (sensitive to outliers)

---

### ğŸ“ˆ Linear Regression

**ğŸ“ Location:** [`algorithms/linear_regression_algorithm/`](algorithms/linear_regression_algorithm/)

**ğŸ“ Algorithm Type:** Supervised Learning - Regression

#### ğŸ” Algorithm Overview

Linear Regression finds the best-fitting straight line through data points using gradient descent optimization. It models the relationship between features and continuous target variables.

#### ğŸ”§ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ“Š **Gradient Descent** | Custom optimization implementation |
| âš™ï¸ **Configurable Learning** | Adjustable learning rate and iterations |
| ğŸ“ **Pure Mathematics** | No hidden abstractions, pure NumPy |
| ğŸ“ˆ **MSE Evaluation** | Built-in performance measurement |
| ğŸ¯ **Regression Focus** | Perfect for continuous predictions |

#### ğŸ“ˆ Performance Metrics

**Tested on Synthetic Dataset:**
- **Dataset**: 100 samples, 1 feature + noise
- **Optimization**: 1000 iterations, learning rate 0.1
- **Metric**: Mean Squared Error (MSE)
- **Convergence**: Typically converges within 500 iterations

#### ğŸ’¡ When to Use Linear Regression

âœ… **Good for:**
- Linear relationships between features and target
- Continuous value prediction
- When interpretability is crucial
- Baseline model for comparison

âŒ **Avoid when:**
- Complex non-linear relationships
- Categorical target variables
- When overfitting is a major concern

---

### ğŸ§  Logistic Regression

**ğŸ“ Location:** [`algorithms/logistic_regression_algorithm/`](algorithms/logistic_regression_algorithm/)

**ğŸ“ Algorithm Type:** Supervised Learning - Binary Classification

#### ğŸ” Algorithm Overview

Logistic Regression uses the sigmoid function to transform linear combinations into probabilities, making it perfect for binary classification. It combines linear modeling with probability theory to make intelligent yes/no decisions.

#### ğŸ”§ Key Features

| Feature | Description |
|---------|-------------|
| ğŸŒŠ **Sigmoid Activation** | Converts linear outputs to probabilities (0-1) |
| ğŸ¯ **Binary Classification** | Perfect for yes/no, spam/ham, cancer/benign decisions |
| ğŸ“Š **Gradient Descent** | Custom optimization with cross-entropy loss |
| ğŸ”¬ **Medical Applications** | Tested on real breast cancer detection data |
| ğŸ“ˆ **Probability Output** | Get confidence scores, not just predictions |

#### ğŸ“ˆ Performance Metrics

**Tested on Breast Cancer Dataset:**
- **Dataset**: 569 patients, 30 tumor characteristics
- **Accuracy**: ~91% classification accuracy
- **Classes**: Malignant vs Benign tumors
- **Optimization**: 1000 iterations, learning rate 0.001
- **Medical Relevance**: Can assist in cancer diagnosis

#### ğŸ’¡ When to Use Logistic Regression

âœ… **Good for:**
- Binary classification problems
- When you need probability estimates
- Linear decision boundaries
- Fast inference requirements
- Interpretable medical/business models

âŒ **Avoid when:**
- Multi-class problems (without extensions)
- Complex non-linear relationships
- Perfect class separation exists
- Very high-dimensional sparse data

---

### ğŸ§  Naive Bayes

**ğŸ“ Location:** [`algorithms/naive_bayes_algorithm/`](algorithms/naive_bayes_algorithm/)

**ğŸ“ Algorithm Type:** Supervised Learning - Probabilistic Classification

#### ğŸ” Algorithm Overview

Naive Bayes is a probabilistic classifier based on applying Bayes' theorem with the "naive" assumption of conditional independence between features. Despite its simplicity, it's surprisingly effective for many classification tasks!

#### ğŸ”§ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ² **Probabilistic Foundation** | Uses Bayes' theorem for decision making |
| ğŸ“Š **Statistical Learning** | Calculates mean and variance for each feature |
| ğŸš€ **Fast Training** | No iterative optimization - just statistical calculations |
| ğŸ¯ **Multi-class Support** | Naturally handles multiple classes |
| ğŸ“ˆ **Real Testing** | Validated on synthetic classification datasets |

#### ğŸ“ˆ Performance Metrics

**Tested on Synthetic Dataset:**
- **Dataset**: 1000 samples, 10 features, 2 classes
- **Accuracy**: ~92.5% classification accuracy
- **Training Time**: Instant (no iterative training)
- **Memory**: Very efficient - stores only statistical summaries

#### ğŸ’¡ When to Use Naive Bayes

âœ… **Good for:**
- Text classification (spam detection, sentiment analysis)
- Small datasets with limited training data
- Fast training and prediction requirements
- Multi-class classification problems
- High-dimensional data

âŒ **Avoid when:**
- Features are strongly correlated
- Non-normal feature distributions
- Complex feature interactions are important

Let's dive into the actual implementations! Here's how each algorithm works in practice:

### ğŸ¯ KNN Implementation Walkthrough

#### Core Distance Function
``python
# ğŸ“ File: algorithms/knn_algorithm/knn.py
def euclidean_distance(x1, x2):
    return np.sqrt(np.sum((x1-x2)**2))
```
**What it does:** Calculates the straight-line distance between two points in multi-dimensional space.

#### The KNN Class
``python
class KNN:
    def __init__(self, k) -> None:
        self.k = k  # Number of neighbors to consider

    def fit(self, X, Y):
        # ğŸ“š Lazy learning - just store the training data!
        self.X_train = X
        self.Y_train = Y

    def predict(self, X):
        # ğŸ” Predict for multiple samples
        predictions = [self._predict(x) for x in X]
        return predictions
    
    def _predict(self, x):
        # ğŸ“ Step 1: Calculate distance to all training points
        distance = [euclidean_distance(x, x_train) for x_train in self.X_train]
        
        # ğŸ“Š Step 2: Find k nearest neighbors
        k_indices = np.argsort(distance)[:self.k]
        k_nearest_labels = [self.Y_train[i] for i in k_indices]
        
        # ğŸ—³ï¸ Step 3: Democratic voting - most common class wins!
        most_common = np.bincount(k_nearest_labels).argmax()
        return most_common
```

#### KNN in Action
``python
# ğŸ“ File: algorithms/knn_algorithm/knn_test.py
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# ğŸŒº Load the famous Iris dataset
iris = datasets.load_iris()
X_train, X_test, Y_train, Y_test = train_test_split(
    iris.data, iris.target, random_state=0, test_size=0.2
)

# ğŸ¯ Create and train KNN classifier
clf = KNN(k=5)
clf.fit(X_train, Y_train)

# ğŸ”® Make predictions and check accuracy
y_pred = clf.predict(X_test)
print(accuracy_score(Y_test, y_pred))  # Expected: ~0.97 (97% accuracy!)
```

### ğŸ“ˆ Linear Regression Implementation Walkthrough

#### The Linear Regression Class
``python
# ğŸ“ File: algorithms/linear_regression_algorithm/linear_regression.py
class LinearRegression:
    def __init__(self, lr, n_iterations):
        self.lr = lr                    # ğŸƒ Learning rate - how big steps to take
        self.n_iterations = n_iterations # ğŸ”„ How many times to improve
        self.weights = None             # ğŸ“ˆ The slope(s) of our line
        self.bias = None               # ğŸ“ˆ The y-intercept of our line

    def fit(self, X, Y):
        # ğŸ¯ Initialize parameters
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)  # Start with zero weights
        self.bias = 0                        # Start with zero bias
        
        # ğŸ”„ Gradient descent optimization loop
        for i in range(self.n_iterations):
            # ğŸ”® Step 1: Make predictions with current weights
            y_predict = np.dot(X, self.weights) + self.bias
            
            # ğŸ“‰ Step 2: Calculate gradients (how to improve)
            dw = (1/n_samples) * np.dot(X.T, (y_predict - Y))
            db = (1/n_samples) * np.sum(y_predict - Y)
            
            # ğŸ‘† Step 3: Update parameters (take a step toward better solution)
            self.weights -= self.lr * dw
            self.bias -= self.lr * db

    def predict(self, X):
        # ğŸ”® Make predictions with learned parameters
        y_predict = np.dot(X, self.weights) + self.bias
        return y_predict
```

#### Linear Regression in Action
``python
# ğŸ“ File: algorithms/linear_regression_algorithm/linear_regression_test.py
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import datasets

# ğŸ“Š Generate synthetic dataset with known relationship
X, Y = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=4)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1234)

# ğŸ¯ Create and train the regressor
regressor = LinearRegression(lr=0.1, n_iterations=1000)
regressor.fit(X_train, Y_train)  # ğŸ’ª Watch it learn!

# ğŸ”® Test how well it learned
predictions = regressor.predict(X_test)

# ğŸ“ˆ Calculate Mean Squared Error
def MSE(y_true, y_pred):
    return np.mean((y_true - y_pred)**2)

mse_value = MSE(Y_test, predictions)
print("MSE Value is:", mse_value)  # Expected: 200-500 (lower is better!)
```

### ğŸ§  Logistic Regression Implementation Walkthrough

#### The Logistic Regression Class
```python
# ğŸ“ File: algorithms/logistic_regression_algorithm/logistic_regression.py
class LogisticRegression:
    def __init__(self, lr, n_iters):
        self.lr = lr                    # ğŸƒ Learning rate - step size for optimization
        self.n_iters = n_iters          # ğŸ”„ Number of training iterations
        self.weights = None             # ğŸ“ˆ Feature weights (learned parameters)
        self.bias = None               # ğŸ“ˆ Bias term (y-intercept equivalent)

    def fit(self, X, Y):
        # ğŸ¯ Initialize parameters
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)  # Start with zero weights
        self.bias = 0                        # Start with zero bias
        
        # ğŸ”„ Gradient descent optimization loop
        for i in range(self.n_iters):
            # ğŸ”® Step 1: Linear transformation
            linear_model = np.dot(X, self.weights) + self.bias
            
            # ğŸŒŠ Step 2: Sigmoid activation (the magic happens here!)
            y_predict = self._segmoid(linear_model)
            
            # ğŸ“‰ Step 3: Calculate gradients (how to improve)
            dw = (1/n_samples) * np.dot(X.T, (y_predict - Y))
            db = (1/n_samples) * np.sum(y_predict - Y)
            
            # ğŸ‘† Step 4: Update parameters (take a step toward better solution)
            self.weights -= self.lr * dw
            self.bias -= self.lr * db

    def predict(self, X):
        # ğŸ”® Get probabilities
        linear_model = np.dot(X, self.weights) + self.bias
        y_predict = self._segmoid(linear_model)
        
        # ğŸ¯ Convert probabilities to binary predictions
        y_predict_class = [1 if i > 0.5 else 0 for i in y_predict]
        return y_predict_class
    
    def _segmoid(self, s):
        # ğŸŒŠ Sigmoid function: converts any real number to (0, 1)
        return (1/(1+np.exp(-s)))  # Note: Fixed the sign for correct implementation
```

#### Logistic Regression in Action - Cancer Detection!
```python
# ğŸ“ File: algorithms/logistic_regression_algorithm/logistic_regression_test.py
import numpy as np
from sklearn.model_selection import train_test_split 
from sklearn.metrics import accuracy_score
from sklearn import datasets

# ğŸ”¬ Load real medical data - breast cancer dataset
data_set = datasets.load_breast_cancer()
X, Y = data_set.data, data_set.target
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1234)

# ğŸ¯ Create and train the logistic regression classifier
logistic_reg = LogisticRegression(lr=0.001, n_iters=1000)
logistic_reg.fit(X_train, Y_train)  # ğŸ’ª Watch it learn to detect cancer!

# ğŸ”® Test how well it learned
predictions = logistic_reg.predict(X_test)

# ğŸ“ˆ Calculate accuracy
def accuracy(y_true, y_pred):
    accuracy = np.sum(y_true == y_pred) / len(y_true)
    return accuracy

print('The accuracy of the model is:', accuracy(Y_test, predictions))  # Expected: ~0.91 (91% accuracy!)
```

### ğŸ§  Naive Bayes Implementation Walkthrough

#### The Naive Bayes Class
```python
# ğŸ“ File: algorithms/naive_bayes_algorithm/naive_bayes.py
class NaiveBayes:
    def fit(self, X, Y):
        n_samples, n_features = X.shape
        self._classes = np.unique(Y)
        n_classes = len(self._classes)

        # ğŸ¯ Initialize statistics storage
        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)
        self._var = np.zeros((n_classes, n_features), dtype=np.float64)
        self._priors = np.zeros((n_classes), dtype=np.float64)

        # ğŸ“Š Calculate statistics for each class
        for idx, c in enumerate(self._classes):
            X_c = X[Y == c]  # Get samples for this class
            self._mean[idx, :] = X_c.mean(axis=0)    # Feature means
            self._var[idx, :] = X_c.var(axis=0)      # Feature variances
            self._priors[idx] = X_c.shape[0] / float(n_samples)  # Class probability
    
    def predict(self, X):
        y_predict = [self._predict(x) for x in X]
        return np.array(y_predict)
    
    def _predict(self, x):
        posteriors = []
        # ğŸ² Calculate posterior probability for each class
        for idx, c in enumerate(self._classes):
            prior = np.log(self._priors[idx])  # P(class)
            posterior = np.sum(np.log(self._pdf(idx, x)))  # P(features|class)
            posterior = prior + posterior  # P(class|features) âˆ P(class) Ã— P(features|class)
            posteriors.append(posterior)
        return self._classes[np.argmax(posteriors)]
    
    def _pdf(self, class_idx, x):
        # ğŸŒŠ Gaussian probability density function
        mean = self._mean[class_idx]
        var = self._var[class_idx]
        numerator = np.exp(-((x - mean) ** 2) / (2 * var))
        denominator = np.sqrt(2 * np.pi * var)
        return numerator / denominator
```

#### Naive Bayes in Action - Statistical Classification!
```python
# ğŸ“ File: algorithms/naive_bayes_algorithm/naive_bayes_test.py
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import datasets

# ğŸ² Generate synthetic binary classification data
X, Y = datasets.make_classification(
    n_samples=1000, n_features=10, n_classes=2, random_state=123
)
X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y, test_size=0.2, random_state=123
)

# ğŸ¯ Create and train Naive Bayes classifier  
nb = NaiveBayes()
nb.fit(X_train, Y_train)  # ğŸ’ª Watch it learn statistics!

# ğŸ”® Test probabilistic predictions
predictions = nb.predict(X_test)

# ğŸ“ˆ Calculate accuracy
def accuracy(y_true, y_pred):
    return np.sum(y_true == y_pred) / len(y_true)

print('Naive Bayes classification accuracy', accuracy(Y_test, predictions))  # Expected: ~0.925 (92.5% accuracy!)
```

---

### ğŸŒ³ Decision Tree

**ğŸ“ Location:** [`algorithms/decision_tree_algorithm/`](algorithms/decision_tree_algorithm/)

**ğŸ“ Algorithm Type:** Supervised Learning - Classification

#### ğŸ” Algorithm Overview

Decision Trees are one of the most intuitive machine learning algorithms that work exactly like human decision making by asking a series of yes/no questions to reach a conclusion. They use information theory to automatically determine the best questions to ask.

#### ğŸ”§ Key Features

| Feature | Description |
|---------|-------------|
| âœ¨ **Information Gain** | Uses entropy to find optimal splits |
| ğŸŒ³ **Tree Structure** | Builds hierarchical decision rules |
| ğŸ“Š **Entropy Calculation** | Measures data "purity" and information content |
| ğŸ”„ **Recursive Splitting** | Grows tree by repeatedly finding best questions |
| ğŸ¯ **Binary Decisions** | Each node makes a simple yes/no decision |
| ğŸ“Š **Real Testing** | Validated on breast cancer medical dataset |

#### ğŸ“ˆ Performance Metrics

**Tested on Breast Cancer Dataset:**
- **Dataset Size**: 569 patients, 30 tumor characteristics
- **Accuracy**: ~91-92% classification accuracy
- **Classes**: Malignant vs Benign tumors
- **Tree Depth**: Configurable (default max_depth=10)
- **Medical Impact**: Can assist doctors in cancer diagnosis! ğŸ¥

#### ğŸ’¡ When to Use Decision Trees

âœ… **Good for:**
- Interpretable models (can explain every decision)
- Mixed data types (numerical and categorical)
- Non-linear relationships
- Feature selection (automatically identifies important features)
- Medical diagnosis (doctors can follow the logic)

âŒ **Avoid when:**
- Overfitting is a major concern
- Data is very noisy
- Linear relationships dominate
- You need very stable models

---

### ğŸŒ² Random Forest

**ğŸ“ Location:** [`algorithms/random_forest_algorithm/`](algorithms/random_forest_algorithm/)

**ğŸ“ Algorithm Type:** Supervised Learning - Ensemble Classification

#### ğŸ” Algorithm Overview

Random Forest is a powerful ensemble learning algorithm that combines multiple decision trees to create a robust, accurate classifier. It uses bootstrap sampling (bagging) and feature randomness to reduce overfitting and improve generalization.

#### ğŸ”§ Key Features

| Feature | Description |
|---------|-------------|
| ğŸŒ² **Ensemble Learning** | Combines multiple decision trees for better performance |
| ğŸ² **Bootstrap Sampling** | Uses random sampling with replacement (bagging) |
| ğŸ”€ **Feature Randomness** | Each tree considers random subset of features |
| ğŸ—³ï¸ **Democratic Voting** | Final prediction based on majority vote |
| ğŸ›¡ï¸ **Overfitting Resistance** | Ensemble reduces variance and overfitting |
| ğŸ¥ **Medical Testing** | Validated on breast cancer diagnosis dataset |

#### ğŸ“ˆ Performance Metrics

**Tested on Breast Cancer Dataset:**
- **Dataset Size**: 569 patients, 30 tumor characteristics
- **Accuracy**: ~89% classification accuracy
- **Ensemble Size**: 3 trees (configurable up to hundreds)
- **Robustness**: More stable than single decision trees
- **Medical Impact**: Ensemble approach increases diagnostic confidence! ğŸ¥

#### ğŸ’¡ When to Use Random Forest

âœ… **Good for:**
- Almost any classification problem (very versatile)
- Noisy datasets (robust to outliers)
- Feature importance analysis
- When you need good performance quickly
- Mixed data types (numerical and categorical)
- Medium to large datasets

âŒ **Avoid when:**
- Very small datasets (ensemble overhead not worth it)
- Real-time predictions needed (slower than single models)
- Memory is severely constrained
- Linear relationships dominate

---

### ğŸ“Š Principal Component Analysis (PCA)

**ğŸ“ Location:** [`algorithms/pca_algorithm/`](algorithms/pca_algorithm/)

**ğŸ“ Algorithm Type:** Unsupervised Learning - Dimensionality Reduction

#### ğŸ” Algorithm Overview

Principal Component Analysis (PCA) is a powerful dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while preserving maximum variance. It finds the "best camera angles" to capture the most important features of your data, making complex datasets visualizable and more manageable.

#### ğŸ”§ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ“Š **Eigendecomposition** | Uses mathematical eigenvalues and eigenvectors for optimal projections |
| ğŸ¯ **Variance Preservation** | Maintains maximum data spread in reduced dimensions |
| ğŸ“ˆ **Data Visualization** | Transforms high-D data into 2D/3D for plotting |
| ğŸ§® **Pure Mathematics** | Built from scratch using only NumPy linear algebra |
| ğŸŒº **Real Testing** | Validated on famous Iris dataset (4D â†’ 2D reduction) |
| âš¡ **Efficient Transform** | Fast matrix operations for data projection |

#### ğŸ“ˆ Performance Metrics

**Tested on Iris Dataset:**
- **Original Dimensions**: 4 features (sepal & petal measurements)
- **Reduced Dimensions**: 2 principal components for visualization
- **Data Preservation**: Maintains key patterns and species clusters
- **Visualization**: Clear separation of iris species in 2D space
- **Speed**: Instant transformation for small-medium datasets

#### ğŸ’¡ When to Use PCA

âœ… **Good for:**
- High-dimensional data visualization (scatter plots, exploration)
- Data compression and noise reduction
- Feature extraction before other ML algorithms
- Exploratory data analysis and pattern discovery
- Image compression and computer vision preprocessing
- Removing correlated features and multicollinearity

âŒ **Avoid when:**
- You need interpretable original features
- Data is already low-dimensional
- Linear relationships don't capture data structure
- Sparse data (many zeros) where non-linear methods work better

#### ğŸ¯ PCA Implementation Walkthrough

**The PCA Class:**
```python
# ğŸ“ File: algorithms/pca_algorithm/pca.py
class PCA:
    def __init__(self, n_components):
        self.n_components = n_components  # Number of dimensions to reduce to
        self.components = None           # Principal components (directions)
        self.mean = None                # Data mean for centering
    
    def fit(self, X):
        # ğŸ“Š Step 1: Center the data (subtract mean)
        self.mean = np.mean(X, axis=0)
        X = X - self.mean
        
        # ğŸ§® Step 2: Calculate covariance matrix
        cov = np.cov(X.T)
        
        # âš¡ Step 3: Find eigenvalues and eigenvectors
        eigenvalues, eigenvectors = np.linalg.eig(cov)
        eigenvectors = eigenvectors.T
        
        # ğŸ“ˆ Step 4: Sort by importance (eigenvalue magnitude)
        idxs = np.argsort(eigenvalues)[::-1]
        eigenvectors = eigenvectors[idxs]
        eigenvalues = eigenvalues[idxs]
        
        # ğŸ¯ Step 5: Select top components
        self.components = eigenvectors[:self.n_components]
    
    def transform(self, X):
        # âœ… Safety check
        if self.components is None:
            raise ValueError("PCA model has not been fitted yet. Call fit() first.")
        
        # ğŸ“Š Center and project data
        X = X - self.mean
        return np.dot(X, self.components.T)
```

**PCA in Action - Iris Visualization:**
```python
# ğŸ“ File: algorithms/pca_algorithm/pca_test.py
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

# ğŸŒº Load famous Iris dataset (4D)
data = datasets.load_iris()
X = data.data  # 4 features: sepal length, sepal width, petal length, petal width
y = data.target  # 3 species: setosa, versicolor, virginica

# ğŸ¯ Apply PCA for 2D visualization
from pca import PCA
pca = PCA(2)  # Reduce from 4D to 2D
pca.fit(X)
X_projected = pca.transform(X)

print("Shape of X:", X.shape)              # (150, 4) - Original 4D
print("Shape of transformed X:", X_projected.shape)  # (150, 2) - Reduced 2D

# ğŸ“ˆ Visualize in 2D space
x1 = X_projected[:, 0]  # First principal component
x2 = X_projected[:, 1]  # Second principal component

plt.scatter(x1, x2, c=y, edgecolor="none", alpha=0.8, 
           cmap=plt.cm.get_cmap("viridis", 3))
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.colorbar()
plt.show()
# Result: Beautiful 2D visualization showing clear species clusters! ğŸŒº
```

#### ğŸ§  Mathematical Foundation

**Core PCA Mathematics:**

1. **Data Centering:**
   ```
   X_centered = X - Î¼
   ```
   Where Î¼ is the mean vector

2. **Covariance Matrix:**
   ```
   C = (1/n) Ã— X_centered^T Ã— X_centered
   ```
   Captures how features vary together

3. **Eigendecomposition:**
   ```
   C Ã— v = Î» Ã— v
   ```
   Where v = eigenvectors (principal components), Î» = eigenvalues (importance)

4. **Data Transformation:**
   ```
   X_reduced = X_centered Ã— V^T
   ```
   Where V contains the top k eigenvectors

**What the Math Means:**
- **Eigenvectors** = directions of maximum variance in data
- **Eigenvalues** = amount of variance captured in each direction
- **Principal Components** = new coordinate system aligned with data patterns
- **Projection** = expressing data in the new reduced coordinate system

#### ğŸ“ Key Learning Insights

**From PCA Implementation:**
- ğŸ§® **Linear Algebra Magic**: Eigendecomposition reveals data structure
- ğŸ“Š **Variance is Information**: Directions with high variance contain important patterns
- ğŸ¯ **Dimensionality vs Information**: We can reduce dimensions while keeping essential information
- ğŸ“ˆ **Visualization Power**: High-dimensional data becomes interpretable in 2D/3D
- âš¡ **Mathematical Elegance**: Complex dimensionality reduction achieved through pure linear algebra

#### ğŸŒŸ Real-World Applications

- **ğŸ–¼ï¸ Image Compression**: Reduce image file sizes while maintaining visual quality
- **ğŸ“Š Data Visualization**: Plot high-dimensional datasets in 2D/3D for human interpretation
- **ğŸ§¬ Genomics**: Analyze gene expression patterns across thousands of genes
- **ğŸ“ˆ Finance**: Identify key market factors from hundreds of stock prices
- **ğŸ¤– Machine Learning**: Feature extraction and preprocessing for other algorithms
- **ğŸ“¸ Computer Vision**: Face recognition and image processing applications

### ğŸ† What Makes These Implementations Special

| Aspect | Our KNN | Our Linear Regression | Our Logistic Regression | Our Naive Bayes |
|--------|---------|----------------------|------------------------|-----------------|
| **ğŸ§® Simplicity** | Pure intuition - just look at neighbors! | Clear math - find the best line! | Sigmoid magic - probabilities made simple! | Pure statistics - calculate and decide! |
| **ğŸ“š Educational** | See every distance calculation | Watch gradient descent optimize | Observe linear-to-probability transformation | Learn Bayes' theorem in action |
| **ğŸ”§ Customizable** | Easy to change k value | Tune learning rate and iterations | Adjust learning parameters and see impact | Modify probability distributions |
| **ğŸ“ˆ Performance** | 97% accuracy on Iris | Low MSE on synthetic data | 91% accuracy on cancer detection | 92.5% accuracy on synthetic data |
| **ğŸš€ Ready to Use** | Import and classify! | Import and predict! | Import and get probabilities! | Import and classify probabilistically! |

### ğŸ“ Key Learning Moments

**From KNN Implementation:**
- ğŸ” **Distance matters**: How we measure similarity affects results
- ğŸ—³ï¸ **Democracy works**: Majority voting is powerful for classification
- ğŸ“š **Lazy learning**: Sometimes storing examples is better than complex training

**From Linear Regression Implementation:**
- ğŸ“ˆ **Gradients guide us**: Math tells us which direction improves performance
- ğŸ”„ **Iteration improves**: Each step gets us closer to the optimal solution  
- âš™ï¸ **Parameters matter**: Learning rate and iterations significantly impact results

**From Logistic Regression Implementation:**
- ğŸŒŠ **Sigmoid transforms**: How to convert any number to a probability
- ğŸ§  **Classification magic**: Binary decisions with confidence scores
- ğŸ¯ **Medical AI**: Real-world applications in healthcare and diagnostics

**From Naive Bayes Implementation:**
- ğŸ² **Bayes' theorem**: The foundation of probabilistic reasoning in AI
- ğŸ“Š **Statistical learning**: How mean and variance capture data patterns
- âš¡ **Instant training**: No optimization needed - just pure statistics!
- ğŸ¯ **Independence assumption**: Sometimes "naive" assumptions work brilliantly

---

## âš™ï¸ Installation & Setup

### ğŸ“¾ Prerequisites

- **Python 3.12+** (recommended)
- **Git** for cloning the repository
- **pip** or **uv** for package management

### ğŸš€ Quick Installation

```
# 1. Clone the repository
git clone <your-repo-url>
cd ML-Algorithms-From-Scratch

# 2. Install dependencies (choose one method)

# Method A: Using pip
pip install -r requirements.txt

# Method B: Using uv (recommended - faster)
uv sync
```

### ğŸ§ª Verify Installation

```bash
# Test KNN Algorithm
cd algorithms/knn_algorithm
python knn_test.py
# Expected output: Accuracy score around 0.97

# Test Linear Regression
cd ../linear_regression_algorithm
python linear_regression_test.py
# Expected output: MSE value showing model performance

# Test Logistic Regression
cd ../logistic_regression_algorithm
python logistic_regression_test.py
# Expected output: Accuracy around 0.91 for cancer detection

# Test Naive Bayes
cd ../naive_bayes_algorithm
python naive_bayes_test.py
# Expected output: Accuracy around 0.925 for classification

# Test Decision Tree
cd ../decision_tree_algorithm
python decision_tree_test.py
# Expected output: Accuracy around 0.91-0.92 for cancer detection

# Test Random Forest
cd ../random_forest_algorithm
python random_forest_test.py
# Expected output: Accuracy around 0.89 for ensemble cancer detection

# Test PCA
cd ../pca_algorithm
python pca_test.py
# Expected output: Beautiful 2D visualization of Iris dataset clusters
```

### ğŸ† Real Performance Results

Here's what you can expect when running the algorithms:

#### ğŸ¯ KNN Results (Iris Dataset)
```
ğŸŒº Running: python knn_test.py
ğŸ“„ Dataset: 150 iris flowers, 4 features, 3 species
ğŸ¯ Classifier: KNN with k=5 neighbors
ğŸ“ˆ Result: 0.9666666666666667
ğŸ‰ That's 97% accuracy - excellent performance!
```

#### ğŸ“ˆ Linear Regression Results (Synthetic Data)
```
ğŸš€ Running: python linear_regression_test.py
ğŸ“„ Dataset: 100 samples with linear relationship + noise
ğŸ¯ Regressor: 1000 iterations, learning rate 0.1
ğŸ“ˆ Result: MSE Value is: ~350-450
ğŸ‰ Low error - the algorithm learned the pattern!
```

#### ğŸ§  Logistic Regression Results (Medical Data)
```
ğŸ”¬ Running: python logistic_regression_test.py
ğŸ“„ Dataset: 569 patients, 30 tumor characteristics
ğŸ¯ Classifier: 1000 iterations, learning rate 0.001
ğŸ“ˆ Result: The accuracy of the model is: 0.912...
ğŸ‰ That's 91% accuracy in cancer detection - potentially life-saving!
```

#### ğŸ§  Naive Bayes Results (Synthetic Data)
```
ğŸ² Running: python naive_bayes_test.py
ğŸ“„ Dataset: 1000 samples, 10 features, 2 classes
ğŸ¯ Classifier: Probabilistic Bayes classification
ğŸ“ˆ Result: Naive Bayes classification accuracy 0.925
ğŸ‰ That's 92.5% accuracy with instant training!
```

#### ğŸŒ³ Decision Tree Results (Medical Data)
```
ğŸŒ³ Running: python decision_tree_test.py
ğŸ“„ Dataset: 569 patients, 30 tumor characteristics
ğŸ¯ Classifier: Decision Tree with max_depth=10
ğŸ“ˆ Result: Accuracy: 0.9210526315789473
ğŸ‰ That's 92% accuracy - excellent interpretable performance!
```

#### ğŸŒ² Random Forest Results (Ensemble Medical Data)
```
ğŸŒ² Running: python random_forest_test.py
ğŸ“„ Dataset: 569 patients, 30 tumor characteristics  
ğŸ¯ Classifier: Random Forest with 3 trees, max_depth=10
ğŸ“ˆ Result: Accuracy: 0.8947368421052632
ğŸ‰ That's 89% accuracy - robust ensemble performance!
```

#### ğŸ“Š PCA Results (Iris Visualization)
```
ğŸŒº Running: python pca_test.py
ğŸ“„ Dataset: 150 iris flowers, 4 features (sepal & petal measurements)
ğŸ¯ Transformer: PCA reducing from 4D to 2D
ğŸ“ˆ Result: Shape of X: (150, 4) â†’ Shape of transformed X: (150, 2)
ğŸ‰ Perfect dimensionality reduction with beautiful species clustering!
```

**What this means:**
- ğŸ¯ **KNN**: Out of 30 test flowers, it correctly identified ~29 species
- ğŸ“ˆ **Linear Regression**: The predicted values are very close to actual values
- ğŸ”¬ **Logistic Regression**: Out of 114 cancer cases, it correctly diagnosed ~104 patients
- ğŸ§  **Naive Bayes**: Out of 200 test samples, it correctly classified ~185 using pure statistics
- ğŸŒ³ **Decision Tree**: Out of 114 cancer cases, it correctly diagnosed ~105 with clear decision rules
- ğŸŒ² **Random Forest**: Out of 114 cancer cases, it correctly diagnosed ~102 using ensemble wisdom
- ğŸ“Š **PCA**: Successfully transformed 4D iris data into clear 2D visualization with preserved species patterns
- ğŸ† **All algorithms work great** and demonstrate core ML concepts!

---
## ğŸ“Š Algorithm Comparisons

### ğŸ†š Feature Comparison

| Aspect | KNN | Linear Regression | Logistic Regression | Naive Bayes | Decision Tree | Random Forest | PCA |
|--------|-----|-------------------|--------------------|-------------|---------------|---------------|-----|
| **Type** | Classification | Regression | Binary Classification | Probabilistic Classification | Classification | Ensemble Classification | Dimensionality Reduction |
| **Learning** | Lazy (Instance-based) | Eager (Model-based) | Eager (Model-based) | Eager (Statistical) | Eager (Tree-based) | Eager (Ensemble-based) | Eager (Transform-based) |
| **Training Time** | O(1) - Just stores data | O(n Ã— iterations) | O(n Ã— iterations) | O(n) - Statistical calculations | O(n Ã— log n Ã— features) | O(n_trees Ã— n Ã— log n) | O(n Ã— pÂ²) - Eigendecomposition |
| **Prediction Time** | O(n Ã— d) - Calculate all distances | O(d) - Simple matrix multiplication | O(d) - Linear + sigmoid | O(d) - Probability calculations | O(depth) - Tree traversal | O(n_trees Ã— depth) | O(p Ã— k) - Matrix projection |
| **Memory Usage** | High - Stores all training data | Low - Only weights and bias | Low - Only weights and bias | Low - Only statistical summaries | Medium - Stores tree structure | High - Stores multiple trees | Low - Only principal components |
| **Interpretability** | Medium - Shows similar examples | High - Clear linear relationship | High - Feature importance + probabilities | High - Probabilistic reasoning | Very High - Clear decision rules | Medium - Ensemble of rules | Medium - Linear combinations |
| **Assumptions** | None | Linear relationship exists | Linear decision boundary | Feature independence | None | None | Linear combinations capture variance |
| **Output** | Class labels | Continuous values | Probabilities + binary predictions | Probabilities + class predictions | Class labels + decision path | Class labels + confidence | Transformed feature vectors |
| **Best For** | Complex decision boundaries | Linear relationships | Binary decisions with confidence | Text classification, small data | Interpretable non-linear models | Robust general-purpose classification | Visualization, noise reduction |

### ğŸ† Performance Comparison

| Dataset Type | KNN Performance | Linear Regression | Logistic Regression | Naive Bayes | Decision Tree | Random Forest | PCA |
|--------------|-----------------|-------------------|--------------------|-------------|---------------|---------------|-----|
| **Small datasets** | âœ… Excellent | âœ… Excellent | âœ… Excellent | âœ… Excellent | âœ… Excellent | âœ… Excellent | âœ… Excellent |
| **Large datasets** | âŒ Poor (slow) | âœ… Good (fast) | âœ… Good (fast) | âœ… Good (fast) | âœ… Good (fast) | âœ… Good (parallel trees) | âœ… Good (linear algebra) |
| **High dimensions** | âŒ Curse of dimensionality | âœ… Handles well with regularization | âœ… Handles reasonably well | âœ… Handles well | âœ… Good (automatic feature selection) | âœ… Excellent (feature sampling) | âœ… Excellent (purpose-built for high-D) |
| **Non-linear data** | âœ… Excellent | âŒ Poor | âŒ Poor (linear boundary only) | âŒ Assumes normal distribution | âœ… Excellent | âœ… Excellent | âŒ Linear projections only |
| **Noisy data** | âŒ Sensitive to outliers | âœ… Robust with proper preprocessing | âœ… Robust to moderate noise | âœ… Robust with smoothing | âŒ Can overfit to noise | âœ… Very robust (ensemble averaging) | âœ… Good (noise reduction) |
| **Binary classification** | âœ… Works but overkill | âŒ Not suitable | âœ… Perfect fit | âœ… Great fit | âœ… Excellent | âœ… Excellent | âŒ Not classification algorithm |
| **Multi-class classification** | âœ… Natural fit | âŒ Not suitable | âŒ Needs extensions | âœ… Natural fit | âœ… Natural fit | âœ… Natural fit | âŒ Not classification algorithm |
| **Probability estimates** | âŒ No built-in probabilities | âŒ Not applicable | âœ… Natural probability output | âœ… Natural probability output | âŒ No built-in probabilities | âœ… Vote-based confidence | âŒ Not applicable |
| **Text classification** | âŒ Poor for text | âŒ Not suitable | âŒ Needs feature engineering | âœ… Excellent | âœ… Good with proper encoding | âœ… Good with proper encoding | âœ… Good for preprocessing |
| **Interpretability** | âœ… Shows examples | âœ… Linear equation | âœ… Feature weights | âœ… Probabilistic reasoning | âœ… Clear decision rules | ğŸŸ¡ Ensemble of rules (less clear) | ğŸŸ¡ Linear combinations |
| **Overfitting resistance** | âœ… Generally robust | âŒ Can overfit | âŒ Can overfit | âœ… Good | âŒ Prone to overfitting | âœ… Very resistant (key strength) | âœ… Good (dimensionality reduction) |
| **Data visualization** | âŒ Not visualization tool | âŒ Linear plots only | âŒ Decision boundaries only | âŒ Not visualization tool | âœ… Tree diagrams | âŒ Not visualization tool | âœ… Excellent (purpose-built) |

---

## ğŸ§  Mathematical Foundations

### ğŸ¯ KNN Mathematics

#### Distance Calculation
**Euclidean Distance Formula:**
```
d(xâ‚, xâ‚‚) = âˆš(âˆ‘áµ¢â‚Œâ‚áµˆ (xâ‚áµ¢ - xâ‚‚áµ¢)Â²)
```

**Where:**
- `d(xâ‚, xâ‚‚)` = distance between points xâ‚ and xâ‚‚
- `n` = number of features
- `xâ‚áµ¢, xâ‚‚áµ¢` = values of feature i for points xâ‚ and xâ‚‚

#### Classification Decision
**Majority Voting:**
```
Å· = mode(yâ‚, yâ‚‚, ..., yâ‚–)
```

**Where:**
- `Å·` = predicted class
- `yâ‚, yâ‚‚, ..., yâ‚–` = classes of k nearest neighbors
- `mode()` = most frequent value

#### Algorithm Steps
1. **Calculate distances** from query point to all training points
2. **Sort distances** in ascending order
3. **Select k nearest** neighbors
4. **Vote** - return most common class among k neighbors

### ğŸ“ˆ Linear Regression Mathematics

#### Linear Model
**Hypothesis Function:**
```
hÎ¸(x) = Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + ... + Î¸â‚™xâ‚™
```

**Matrix Form:**
```
Å· = XÎ¸ + b
```

**Where:**
- `Å·` = predictions vector
- `X` = feature matrix (m Ã— n)
- `Î¸` = weights vector (n Ã— 1)
- `b` = bias term (scalar)

#### Cost Function
**Mean Squared Error (MSE):**
```
J(Î¸,b) = (1/2m) Ã— âˆ‘áµ¢â‚Œâ‚áµ (Å·áµ¢ - yáµ¢)Â²
```

**Where:**
- `J(Î¸,b)` = cost function
- `m` = number of training examples
- `Å·áµ¢` = predicted value for example i
- `yáµ¢` = actual value for example i

#### Gradient Descent
**Weight Update:**
```
Î¸ := Î¸ - Î± Ã— (âˆ‚J/âˆ‚Î¸)
```

**Bias Update:**
```
b := b - Î± Ã— (âˆ‚J/âˆ‚b)
```

**Gradients:**
```
âˆ‚J/âˆ‚Î¸ = (1/m) Ã— Xáµ€ Ã— (Å· - y)
âˆ‚J/âˆ‚b = (1/m) Ã— âˆ‘(Å· - y)
```

**Where:**
- `Î±` = learning rate
- `Xáµ€` = transpose of feature matrix

#### Algorithm Steps
1. **Initialize** weights (Î¸) and bias (b) to zero
2. **Forward pass** - calculate predictions: Å· = XÎ¸ + b
3. **Calculate cost** - compute MSE
4. **Backward pass** - calculate gradients
5. **Update parameters** - apply gradient descent
6. **Repeat** steps 2-5 until convergence

### ğŸ§  Logistic Regression Mathematics

#### Logistic Model
**Linear Transformation:**
```
z = Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + ... + Î¸â‚™xâ‚™
```

**Sigmoid Activation:**
```
Ïƒ(z) = 1/(1 + e^(-z))
```

**Matrix Form:**
```
z = XÎ¸ + b
p = Ïƒ(z) = 1/(1 + e^(-(XÎ¸ + b)))
```

**Where:**
- `z` = linear transformation output
- `p` = predicted probabilities vector
- `X` = feature matrix (m Ã— n)
- `Î¸` = weights vector (n Ã— 1)
- `b` = bias term (scalar)
- `Ïƒ` = sigmoid function

#### Cost Function
**Cross-Entropy (Log Loss):**
```
J(Î¸,b) = -(1/m) Ã— Î£[yÂ·log(Ïƒ(z)) + (1-y)Â·log(1-Ïƒ(z))]
```

**Where:**
- `J(Î¸,b)` = cost function
- `m` = number of training examples
- `y` = actual binary labels (0 or 1)
- `Ïƒ(z)` = predicted probabilities

#### Gradient Descent
**Weight Update:**
```
Î¸ := Î¸ - Î± Ã— (âˆ‚J/âˆ‚Î¸)
```

**Bias Update:**
```
b := b - Î± Ã— (âˆ‚J/âˆ‚b)
```

**Gradients:**
```
âˆ‚J/âˆ‚Î¸ = (1/m) Ã— Xáµ€ Ã— (Ïƒ(z) - y)
âˆ‚J/âˆ‚b = (1/m) Ã— Î£(Ïƒ(z) - y)
```

**Where:**
- `Î±` = learning rate
- `Xáµ€` = transpose of feature matrix

#### Algorithm Steps
1. **Initialize** weights (Î¸) and bias (b) to zero
2. **Forward pass** - calculate linear transformation: z = XÎ¸ + b
3. **Sigmoid activation** - convert to probabilities: p = Ïƒ(z)
4. **Calculate cost** - compute cross-entropy loss
5. **Backward pass** - calculate gradients
6. **Update parameters** - apply gradient descent
7. **Repeat** steps 2-6 until convergence
8. **Prediction** - use threshold (typically 0.5) to convert probabilities to binary predictions

### ğŸ§  Naive Bayes Mathematics

#### Bayes' Theorem
**Core Formula:**
```
P(C|X) = P(X|C) Ã— P(C) / P(X)
```

**For classification (ignoring constant P(X)):**
```
P(C|X) âˆ P(X|C) Ã— P(C)
```

**Where:**
- `P(C|X)` = posterior probability (what we want)
- `P(X|C)` = likelihood (probability of features given class)
- `P(C)` = prior probability (how common is this class)
- `P(X)` = evidence (normalizing constant)

#### Naive Independence Assumption
**Feature Independence:**
```
P(X|C) = P(xâ‚|C) Ã— P(xâ‚‚|C) Ã— ... Ã— P(xâ‚™|C)
```

**Where:**
- `X = [xâ‚, xâ‚‚, ..., xâ‚™]` = feature vector
- Each feature is assumed independent given the class

#### Gaussian Probability Density
**For continuous features:**
```
P(xáµ¢|C) = (1/âˆš(2Ï€ÏƒÂ²)) Ã— exp(-(xáµ¢-Î¼)Â²/(2ÏƒÂ²))
```

**Where:**
- `Î¼` = mean of feature i for class C
- `ÏƒÂ²` = variance of feature i for class C
- `xáµ¢` = value of feature i

#### Classification Decision
**Maximum A Posteriori (MAP):**
```
Å· = argmax_c [log P(C) + Î£áµ¢ log P(xáµ¢|C)]
```

**Algorithm Steps**
1. **Calculate priors** - P(C) for each class
2. **Calculate feature statistics** - mean and variance for each feature per class
3. **For prediction** - calculate P(C|X) for each class using Bayes' theorem
4. **Return class** with highest posterior probability

### ğŸ“Š PCA Mathematics

#### Mathematical Foundation
**Data Centering:**
```
X_centered = X - Î¼
```
**Where:**
- `X` = original data matrix (n Ã— p)
- `Î¼` = mean vector (1 Ã— p)
- `X_centered` = mean-centered data

#### Covariance Matrix
**Covariance Calculation:**
```
C = (1/(n-1)) Ã— X_centered^T Ã— X_centered
```
**Where:**
- `C` = covariance matrix (p Ã— p)
- `n` = number of samples
- `p` = number of features

#### Eigendecomposition
**Eigenvalue Problem:**
```
C Ã— v = Î» Ã— v
```
**Where:**
- `v` = eigenvectors (principal component directions)
- `Î»` = eigenvalues (variance explained by each component)
- `C` = covariance matrix

#### Dimensionality Reduction
**Data Transformation:**
```
X_reduced = X_centered Ã— V^T
```
**Where:**
- `V` = matrix of top k eigenvectors (k Ã— p)
- `X_reduced` = transformed data (n Ã— k)
- `k` = number of desired components (k < p)

#### Variance Preservation
**Explained Variance Ratio:**
```
ratio_i = Î»_i / âˆ‘Î»_j
```
**Where:**
- `ratio_i` = proportion of total variance explained by component i
- `Î»_i` = eigenvalue of component i
- `âˆ‘Î»_j` = sum of all eigenvalues

#### Algorithm Steps
1. **Center the data** - subtract mean from each feature
2. **Compute covariance matrix** - measure feature relationships
3. **Find eigenvectors and eigenvalues** - discover principal directions
4. **Sort by eigenvalue magnitude** - rank components by importance
5. **Select top k components** - choose desired dimensionality
6. **Transform data** - project onto new coordinate system

#### Key Mathematical Insights

**What Eigenvectors Represent:**
- **Direction**: Eigenvectors point in directions of maximum variance
- **Orthogonality**: Principal components are perpendicular to each other
- **Linear Combinations**: Each component is a weighted sum of original features

**What Eigenvalues Represent:**
- **Magnitude**: Larger eigenvalues = more important components
- **Variance**: Eigenvalue = amount of variance captured in that direction
- **Information**: Higher eigenvalues preserve more original data information

**PCA Properties:**
- **Optimal**: Minimizes reconstruction error for given number of components
- **Unique**: Solution is mathematically unique (up to sign)
- **Linear**: Only finds linear relationships between features
- **Unsupervised**: Doesn't use target labels, only feature relationships

---
2. **Calculate feature statistics** - mean and variance for each feature per class
3. **For prediction** - calculate posterior for each class using Bayes' theorem
4. **Choose class** with highest posterior probability

---

## ğŸ“ Project Structure

```
ML-Algorithms-From-Scratch/
â”‚
â”œâ”€â”€ ğŸ“ algorithms/                    # Main algorithms directory
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ¯ knn_algorithm/               # K-Nearest Neighbors implementation
â”‚   â”‚   â”œâ”€â”€ ğŸ knn.py                     # Core KNN class implementation
â”‚   â”‚   â”œâ”€â”€ ğŸ§ª knn_test.py               # KNN testing and validation
â”‚   â”‚   â””â”€â”€ ğŸ“š README.md                # KNN comprehensive guide
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ˆ linear_regression_algorithm/ # Linear Regression implementation
â”‚   â”‚   â”œâ”€â”€ ğŸ linear_regression.py       # Core Linear Regression class
â”‚   â”‚   â”œâ”€â”€ ğŸ§ª linear_regression_test.py   # Testing and validation
â”‚   â”‚   â”œâ”€â”€ ğŸš€ main.py                   # Entry point and examples
â”‚   â”‚   â”œâ”€â”€ ğŸ“š README.md                # Linear Regression guide
â”‚   â”‚   â”œâ”€â”€ ğŸ—‚ï¸ .venv/                      # Virtual environment
â”‚   â”‚   â””â”€â”€ âš™ï¸ pyproject.toml             # Local project configuration
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ§  logistic_regression_algorithm/ # Logistic Regression implementation
â”‚       â”œâ”€â”€ ğŸ logistic_regression.py      # Core Logistic Regression class
â”‚       â”œâ”€â”€ ğŸ§ª logistic_regression_test.py  # Medical data testing
â”‚       â””â”€â”€ ğŸ“š README.md                # Logistic Regression guide
â”‚
â”œâ”€â”€ ğŸ“š README.md                      # Comprehensive documentation (this file!)
â”œâ”€â”€ ğŸ“¦ requirements.txt               # Python dependencies
â””â”€â”€ âš™ï¸ pyproject.toml                # Main project configuration
```

### ğŸ“‚ File Descriptions

| File | Purpose | Key Contents |
|------|---------|-------------|
| **`knn.py`** | KNN Implementation | `KNN` class, `euclidean_distance()` function |
| **`knn_test.py`** | KNN Validation | Iris dataset testing, accuracy measurement |
| **`linear_regression.py`** | Linear Regression | `LinearRegression` class, gradient descent |
| **`linear_regression_test.py`** | Regression Testing | Synthetic data testing, MSE calculation |
| **`logistic_regression.py`** | Logistic Regression | `LogisticRegression` class, sigmoid activation |
| **`logistic_regression_test.py`** | Classification Testing | Medical data testing, cancer detection |
| **`naive_bayes.py`** | Naive Bayes | `NaiveBayes` class, Gaussian PDF, Bayes' theorem |
| **`naive_bayes_test.py`** | Probabilistic Testing | Synthetic data testing, statistical classification |
| **`decision_tree.py`** | Decision Tree | `DecisionTree` class, entropy calculation, information gain |
| **`decision_tree_test.py`** | Tree Testing | Medical data testing, cancer diagnosis |
| **`random_forest.py`** | Random Forest | `RandomForest` class, bootstrap sampling, ensemble voting |
| **`random_forest_test.py`** | Ensemble Testing | Medical data testing, ensemble cancer diagnosis |
| **`requirements.txt`** | Dependencies | NumPy, scikit-learn, tqdm, ipykernel |
| **`pyproject.toml`** | Configuration | Project metadata, workspace settings |

---

## ğŸ”® Future Roadmap

### ğŸ¯ Phase 1: Classification Algorithms (In Progress)

- [x] **K-Nearest Neighbors** - âœ… Completed
- [x] **Logistic Regression** - âœ… Completed - Binary classification with sigmoid
- [x] **Naive Bayes** - âœ… Completed - Probabilistic classification using Bayes' theorem
- [x] **Decision Trees** - âœ… Completed - Tree-based learning with information theory

### ğŸ“ˆ Phase 2: Regression Algorithms (In Progress)

- [x] **Linear Regression** - âœ… Completed
- [ ] **Polynomial Regression** - Non-linear relationships
- [ ] **Ridge Regression** - L2 regularization
- [ ] **Lasso Regression** - L1 regularization

### ğŸ”— Phase 4: Unsupervised Learning

- [ ] **K-Means Clustering** - Partitioning algorithm
- [ ] **Hierarchical Clustering** - Tree-based clustering
- [x] **PCA** - Dimensionality reduction

### ğŸ¯ Phase 5: Ensemble Methods (In Progress)

- [x] **Random Forest** - âœ… Completed - Ensemble of decision trees with bootstrap sampling
- [ ] **AdaBoost** - Adaptive boosting
- [ ] **Gradient Boosting** - Sequential improvement

---

---

## ğŸ› Troubleshooting

### ğŸš‘ Common Issues

#### 1. **Import Errors**
```
ModuleNotFoundError: No module named 'algorithms'
```
**Solution:**
```bash
# Make sure you're in the correct directory
cd algorithms/knn_algorithm
python knn_test.py
```

#### 2. **Low KNN Accuracy**
```
Accuracy: 0.600 (Expected: ~0.97)
```
**Solutions:**
- Try different k values: `KNN(k=3)` or `KNN(k=7)`
- Scale your features if needed
- Check data quality

#### 3. **High Linear Regression MSE**
```
MSE: 1500.0 (Expected: 200-500)
```
**Solutions:**
- Lower learning rate: `lr=0.01` instead of `lr=0.1`
- Increase iterations: `n_iterations=2000`
- Scale features if needed



